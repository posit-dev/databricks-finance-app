---
title: "Predicting lending rates with Databricks and tidymodels"
format: html
knitr:
  opts_chunk: 
    eval: false
---

Machine learning algorithms are reshaping financial decision-making, changing how the industry understands and manages financial risk. By analyzing vast amounts of data, these advanced algorithms deliver predictive insights that drive informed decisions and expedited client service. One example is in the consumer credit market, where accurately predicting lending rates is critical for customer acquisition and retention.

Through historical application analysis, machine learning enables the capability to offer potential clients personalized interest rates, quickly. This mitigates the chance of losing the customers to faster alternatives while minimizing loan-associated risks. Efficient yet thorough application processing allows you to maintain a competitive edge in the market.

Financial analysts using Databricks can harness its performance and robust data governance capabilities, particularly when working with common datasets stored in Delta Lake. Analysts can use ODBC to link with a SQL warehouse or employ sparklyr to interface with a Databricks cluster. Once the data is accessible, the [tidymodels framework of packages](https://www.tidymodels.org/) offer modeling and machine learning capabilities. With Posit professional tools, analysts can use these tools in managed environments with their preferred IDEs and schedule, share, and scale their models. Analysts can seamlessly combine the data governance strengths of Databricks with the powerful tools offered by Posit, making their work more productive and streamlined.

In this article, we will use publicly accessible loan applicant data from LendingClub to create a machine learning model. Our goal is to develop a personalized interest rate prediction model tailored to individual criteria. For those with access to Databricks, create the table in your catalog by running the below:

```sql
CREATE TABLE lending_club USING com.databricks.spark.csv OPTIONS(path 'dbfs:/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv', header "true");
```

## Accessing Databricks tables

We begin with loading the packages we will be using.

```{r}
#| message: false
library(dplyr)
library(tidyr)
library(tidymodels)
```

As mentioned above, analysts have several options for accessing Databricks data. For this walkthrough, we will connect using the [odbc package](https://odbc.r-dbi.org/). The Posit Solutions site walks through how to connect with the new [odbc `databricks()` function](https://solutions.posit.co/connections/db/databases/databricks/index.html#using-the-new-odbcdatabricks-function): 

```{r}
con <-
  DBI::dbConnect(odbc::databricks(), 
                 httpPath = Sys.getenv("HTTP_PATH"))
```

This creates a connection in the `con` object. From there, we can navigate to our table.

```{r}
#| eval: false
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) 
```

We can use the {dbplot} package to 

```{r}
library(dbplot)

lendingclub_dat |> 
   dbplot_histogram(int_rate)
```

The `lendingclub_dat` object looks like a data frame, but it is actually a SQL table. With the [dbplyr package](), we can use dplyr syntax to query and wrangle these SQL tables in R. Leaving as much as possible before "collecting" the data means much faster operations.

When first making the connection, some columns are imported as characters even though they should be numeric. Using the `mutate()` function , we can convert them into the right type. We can also use `mutate()` to create some additional variables that may be predictive of interest rate.

```{r}
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |> 
  mutate(
    # Convert these columns into numeric
    across(c(starts_with("annual"), starts_with("dti"), starts_with("inq"),  starts_with("mo"), starts_with("mths"), starts_with("num"), starts_with("open"), starts_with("percent"), starts_with("pct"), starts_with("revol"), starts_with("tot"), "all_util", "il_util", "tax_liens",  "loan_amnt", "installment", "pub_rec_bankruptcies", "num_tl_120dpd_2m", "bc_util", "max_bal_bc", "bc_open_to_buy", "acc_open_past_24mths", "avg_cur_bal", "delinq_2yrs", "pub_rec"), ~ as.numeric(.)),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate the percentage of monthly income the installment payment represents
    installment_pct_inc = case_when(
      application_type == "Individual" ~ installment / (annual_inc / 12),
      .default = installment / (annual_inc_joint / 12)
    ),
    # Calculate the percentage of monthly income the installment payment represents
    adjusted_dti = case_when(
      application_type == "Individual" ~ (loan_amnt + tot_cur_bal) / (annual_inc),
      .default = (loan_amnt + tot_cur_bal) / (annual_inc_joint)
    ),
    #  Calculate utilization on installment accounts excluding mortgage balance
    il_util_ex_mort = case_when(
      total_il_high_credit_limit > 0 ~ total_bal_ex_mort / total_il_high_credit_limit,
      .default = 0
    ),
    # Fill debt to income joint with individual debt to income where missing
    dti_joint = coalesce(dti_joint, dti),
    # Fill annual income joint with individual annual income where missing
    annual_inc_joint = coalesce(annual_inc_joint, annual_inc)) |> 
  collect()
```

Now that we've cleaned up our data, we can take a look at the variables we'd like to use in the model.

```{r}
applicant_numeric <- 
  c("annual_inc","dti","age_earliest_cr","loan_amnt", "installment")
applicant_text <- 
  c("emp_title","title")
applicant_categorical <- c("application_type", "emp_length", "term")
credit_numeric        <- c("acc_open_past_24mths","avg_cur_bal","bc_open_to_buy","bc_util","delinq_2yrs","open_acc","pub_rec","revol_bal","tot_coll_amt","tot_cur_bal","total_acc","total_rev_hi_lim","num_accts_ever_120_pd","num_actv_bc_tl","num_actv_rev_tl","num_bc_sats","num_bc_tl","num_il_tl", "num_rev_tl_bal_gt_0","pct_tl_nvr_dlq","percent_bc_gt_75","tot_hi_cred_lim","total_bal_ex_mort","total_bc_limit","total_il_high_credit_limit","total_rev_hi_lim","all_util", "loan_to_income", "installment_pct_inc","il_util","il_util_ex_mort","total_bal_il","total_cu_tl")
NUMERIC_VARS_QB_20    <- c("inq_last_6mths","mo_sin_old_il_acct", "mo_sin_old_rev_tl_op", "mo_sin_old_rev_tl_op", "mo_sin_rcnt_tl", "mort_acc","num_op_rev_tl","num_rev_accts","num_sats","pub_rec","pub_rec_bankruptcies","tax_liens", "all_util", "loan_to_income")
NUMERIC_VARS_QB_5     <- c("num_tl_120dpd_2m")
NUMERIC_VARS_QB_10    <- c("mths_since_last_delinq","mths_since_last_major_derog","mths_since_last_record","mths_since_rcnt_il","mths_since_recent_bc","mths_since_recent_bc_dlq","mths_since_recent_inq","mths_since_recent_revol_delinq", "num_tl_90g_dpd_24m","num_tl_op_past_12m")
NUMERIC_VARS_QB_50    <- c("installment","bc_open_to_buy","loan_amnt","total_bc_limit","percent_bc_gt_75")
```

## Exploratory analysis

Now that our dataset is ready, we can do investigation of our data.

```{r}

```






