---
title: "Predicting lending rates with Databricks and tidymodels"
format: 
  html:
    css: styles.css
    code-fold: true
    code-summary: "Show the code"
title-block-banner: banner.png
toc: true
filters:
  - shinylive
---

Machine learning algorithms are reshaping financial decision-making, changing how the industry understands and manages financial risk. By analyzing vast amounts of data, these advanced algorithms deliver predictive insights that drive informed decisions and expedited client service. One example is in the consumer credit market, where accurately predicting lending rates is critical for customer acquisition and retention.

Through historical application analysis, machine learning enables the capability to offer potential clients personalized lending rates, quickly. This mitigates the chance of losing the customers to faster alternatives while minimizing loan-associated risks. Efficient but thorough application processing allows you to maintain a competitive edge in the market.

Financial analysts using Databricks can harness its performance and robust data governance capabilities, particularly when working with common datasets stored in Delta Lake. Analysts can use [ODBC](https://odbc.r-dbi.org/) to link with a SQL warehouse or employ [sparklyr](https://spark.posit.co/deployment/databricks-connect.html) to interface with a Databricks cluster.

Once the data is accessible, the [tidymodels framework of packages](https://www.tidymodels.org/) offer modeling and machine learning capabilities. With Posit professional tools, analysts can use these tools in managed environments with their preferred IDEs and schedule, share, and scale their models. Analysts can seamlessly combine the data governance strengths of Databricks with the powerful tools offered by Posit, making their work more productive and streamlined.

In this article, we will use publicly accessible loan applicant data from LendingClub to create a machine learning model.^[Thank you to James Andersen for the blog post Predicting Lending Rates: An Intro to AWS Machine Learning that served as a starting off point for this post.] Our goal is to develop a personalized lending rate prediction model tailored to individual criteria.

## Databricks Delta Lake connection to RStudio

For those with access to Databricks, create the table in your catalog by running the below:

```sql
CREATE TABLE lending_club USING com.databricks.spark.csv OPTIONS(path 'dbfs:/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv', header "true");
```

Let's start by loading the packages that will be integral to our workflow.

```{r}
#| label: setup
#| message: false
# Wrangling packages
library(dplyr)
library(tidyr)

# Visualization packages
library(dbplot)
library(ggplot2)

# Modeling packages
library(ranger)
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(yardstick)

# MLOps
library(pins)
library(plumber)
library(vetiver)
```

As previously mentioned, analysts have several options for accessing Databricks data. In this walkthrough, we'll demonstrate connectivity using the [odbc package](https://odbc.r-dbi.org/). The Posit Solutions site provides detailed guidance on using the new [odbc `databricks()` function](https://solutions.posit.co/connections/db/databases/databricks/index.html#using-the-new-odbcdatabricks-function). Essentially, we need to store a Databricks token and host URL in our R environment. The function itself requires only the HTTP path to the SQL warehouse.

```{r}
#| label: odbc-connection
con <-
  DBI::dbConnect(odbc::databricks(),
                 httpPath = Sys.getenv("HTTP_PATH"))
```

This code snippet establishes a connection using the `con` object. Subsequently, we can use dplyr and dbplyr to navigate to our table. We retrieve the `lendingclub` table using `tbl()` and store it in an object named `lendingclub_dat`.

```{r}
#| label: get-tbl-data
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) 
```

We can use the [dbplot package](https://edgararuiz.github.io/dbplot/) to generate a ggplot without transferring the data into R. This visualization displays the distribution of the variable we aim to predict, lending rate. Loan amounts are plotted on the x-axis, while the frequency of loans at each amount is depicted on the y-axis. It's shown that the majority of loans have an lending rate of less than 20%.

```{r}
#| label: dbplot-int-rate
#| warning: false
lendingclub_dat |>
  mutate(int_rate = as.numeric(REGEXP_REPLACE(int_rate, "%", ""))) |>
  db_compute_bins(int_rate, binwidth = 0.5) |>
  ggplot() +
  geom_col(
    aes(x = int_rate, y = count),
    fill = "#1B909E",
    color = "#1B909E",
    alpha = 0.4
  ) +
labs(title = "Distribution of lending rate",
     x = "lending rate",
     y = "Count") +
  theme_minimal()
```

## Data cleaning and feature engineering

The `lendingclub_dat` object resembles a typical data frame, but it's actually a SQL table. With the [dbplyr package](), we can use dplyr syntax to query and manipulate these SQL tables directly in R. Running operations before "collecting" the data leads to significantly faster performance.

Certain columns are imported as characters instead of their intended numeric type. With the `mutate()` function, we can convert them to the correct type. Additionally, `mutate()` allows us to create supplementary variables that could potentially influence lending rates.

```{r}
#| label: pre-collect-data-clean
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |> 
  mutate(
    # Convert these columns into numeric
    across(c(starts_with("annual"), starts_with("dti"), starts_with("inq"),  starts_with("mo"), starts_with("mths"), starts_with("num"), starts_with("open"), starts_with("percent"), starts_with("pct"), starts_with("revol"), starts_with("tot"), "all_util", "il_util", "tax_liens",  "loan_amnt", "installment", "pub_rec_bankruptcies", "num_tl_120dpd_2m", "bc_util", "max_bal_bc", "bc_open_to_buy", "acc_open_past_24mths", "avg_cur_bal", "delinq_2yrs", "pub_rec"), ~ as.numeric(.)),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate the percentage of monthly income the installment payment represents
    installment_pct_inc = case_when(
      application_type == "Individual" ~ installment / (annual_inc / 12),
      .default = installment / (annual_inc_joint / 12)
    ),
    # Calculate the percentage of monthly income the installment payment represents
    adjusted_dti = case_when(
      application_type == "Individual" ~ (loan_amnt + tot_cur_bal) / (annual_inc),
      .default = (loan_amnt + tot_cur_bal) / (annual_inc_joint)
    ),
    #  Calculate utilization on installment accounts excluding mortgage balance
    il_util_ex_mort = case_when(
      total_il_high_credit_limit > 0 ~ total_bal_ex_mort / total_il_high_credit_limit,
      .default = 0
    ),
    # Fill debt to income joint with individual debt to income where missing
    dti_joint = coalesce(dti_joint, dti),
    # Fill annual income joint with individual annual income where missing
    annual_inc_joint = coalesce(annual_inc_joint, annual_inc)) |> 
  collect()
```

Although we ran these steps before `collect()`, once we have our data frame in R, we gain access to additional advanced R functions that are not available in SQL. This enables us to further refine and clean our data as needed.

```{r}
#| label: post-collect-data-clean
lendingclub_dat_clean <-
  lendingclub_dat |>
  mutate(
    # Missing values for these columns seem most appropriate to fill with zero
    across(c("inq_fi", "dti", "all_util", "percent_bc_gt_75", "il_util", "avg_cur_bal","all_util", "il_util", "inq_last_6mths", "inq_last_12m", "num_tl_120dpd_2m", "open_il_12m", "open_il_24m", "open_rv_12m", "open_rv_24m"), ~ replace_na(., 0)),
    # Missing values for these columns seem most appropriate to fill with the column max
    across(c("mo_sin_old_il_acct", "mths_since_last_major_derog", "mths_since_last_delinq", "mths_since_recent_bc", "mths_since_last_record", "mths_since_rcnt_il", "mths_since_recent_bc", "mths_since_recent_bc_dlq", "mths_since_recent_inq", "mths_since_recent_revol_delinq", "mths_since_recent_revol_delinq"),  ~ replace_na(., max(., na.rm = TRUE))),
    # Remove percent sign
    int_rate = as.numeric(stringr::str_remove(int_rate, "%")),
    # Remove percent sign
    revol_util = as.numeric(stringr::str_remove(revol_util, "%")),
    # Create variable for earliest line of credit
    earliest_cr_line = lubridate::parse_date_time2(paste("01", earliest_cr_line, sep = "-"), "dmy", cutoff_2000 = 50L),
    # Calculate time since earliest line of credit
    age_earliest_cr = lubridate::interval(as.Date(earliest_cr_line), as.Date(lubridate::today())) %/% lubridate::days(1),
    # Convert characters to factors
    across(where(is.character), .fns = as.factor),
    term = trimws(term))
```

With our data cleaned up, it's time to examine the variables we intend to include in the model. We create vectors for conveniently referencing variable categories later if necessary. Then, we compile all the desired columns into one vector named `all_vars` for use in our model.

```{r}
#| label: create-variable-vectors
applicant_numeric <- 
  c("annual_inc","dti","age_earliest_cr","loan_amnt", "installment")
applicant_text <- 
  c("emp_title","title")
applicant_categorical <- 
  c("application_type", "emp_length", "term")
credit_numeric <- 
  c("acc_open_past_24mths","avg_cur_bal","bc_open_to_buy","bc_util","delinq_2yrs","open_acc","pub_rec","revol_bal","tot_coll_amt","tot_cur_bal","total_acc","total_rev_hi_lim","num_accts_ever_120_pd","num_actv_bc_tl","num_actv_rev_tl","num_bc_sats","num_bc_tl","num_il_tl", "num_rev_tl_bal_gt_0","pct_tl_nvr_dlq","percent_bc_gt_75","tot_hi_cred_lim","total_bal_ex_mort","total_bc_limit","total_il_high_credit_limit","total_rev_hi_lim","all_util", "loan_to_income", "installment_pct_inc","il_util","il_util_ex_mort","total_bal_il","total_cu_tl")
NUMERIC_VARS_QB_20  <- 
  c("inq_last_6mths","mo_sin_old_il_acct", "mo_sin_old_rev_tl_op", "mo_sin_old_rev_tl_op", "mo_sin_rcnt_tl", "mort_acc", "num_op_rev_tl", "num_rev_accts", "num_sats", "pub_rec", "pub_rec_bankruptcies", "tax_liens", "all_util", "loan_to_income")
NUMERIC_VARS_QB_5 <- 
  c("num_tl_120dpd_2m")
NUMERIC_VARS_QB_10 <- 
  c("mths_since_last_delinq","mths_since_last_major_derog","mths_since_last_record","mths_since_rcnt_il","mths_since_recent_bc","mths_since_recent_bc_dlq","mths_since_recent_inq","mths_since_recent_revol_delinq", "num_tl_90g_dpd_24m","num_tl_op_past_12m")
NUMERIC_VARS_QB_50 <- 
  c("installment","bc_open_to_buy","loan_amnt","total_bc_limit","percent_bc_gt_75")
mean_impute_vals <- c("bc_util", "num_rev_accts", "bc_open_to_buy", "percent_bc_gt_75", "total_bal_il", "total_il_high_credit_limit", "total_cu_tl")

all_vars <- c(applicant_numeric, applicant_categorical, credit_numeric, NUMERIC_VARS_QB_20, NUMERIC_VARS_QB_5, NUMERIC_VARS_QB_10, NUMERIC_VARS_QB_50)
```

Next, we proceed to select only the relevant columns for our model and remove any missing values in our `int_rate` variable:

```{r}
#| label: create-model-dataset
lendingclub_dat_cols <-
  lendingclub_dat_clean |>
  select(int_rate, all_of(all_vars)) |>
  filter(!is.na(int_rate))
```

To ensure our dataset is free of any missing values, we can run the below:

```{r}
#| label: check-nas
colSums(is.na(lendingclub_dat_cols))
```

We can ensure that no factor columns have fewer than two factors by running the below:

```{r}
#| label: check-factors
lendingclub_dat_cols |>
  select(where(is.factor)) %>%
  select(where( ~ nlevels(.) < 2))
```

## Model creation

With our dataset prepared, we can begin the modeling process. This involves creating train and test datasets, which we'll use to train and evaluate our model.

```{r}
#| label: create-test-train
set.seed(1234)

train_test_split <- initial_split(lendingclub_dat_cols)
lend_train <- training(train_test_split)
lend_test <- testing(train_test_split)
```

Using tidymodels, we can construct a "recipe" detailing the decisions we want to make for our data:

* `step_normalize()`: This recipe step normalizes numeric data to have a standard deviation of one and a mean of zero. Given that our dataset contains numeric values of various units (e.g., dollars, months), normalization ensures that variables with larger value ranges do not disproportionately influence the model.
* `step_impute_mean()`: This recipe step replaces missing values of numeric variables with the mean of those values in the training set. Performing this step now, rather than during the data cleaning phase, prevents information leakage from the testing set into the model. By calculating the mean solely using the training set, we avoid bias when applying it to the testing data during model evaluation.

```{r}
rec_obj <- recipe(int_rate ~ ., data = lend_train) |>
  step_normalize(applicant_numeric, credit_numeric) |>
  step_impute_mean(mean_impute_vals)
```

To verify that the recipe functions as intended, we can run `prep(rec_obj, lend_train) %>% bake(newdata = NULL)`. This will display the data that the workflow will provide to the model.

```{r}
#| warning: false
#| message: false
prep(rec_obj, lend_train) %>% 
  bake(new_data = NULL)
```

Now, we proceed with a linear model. `linear_reg()` specifies a model capable of predicting numeric values from predictors using a linear function. We construct a workflow incorporating our recipe alongside the linear model.

```{r}
lend_linear <- linear_reg()

lend_linear_wflow <-
  workflow() |>
  add_model(lend_linear) |>
  add_recipe(rec_obj)
```

Now, we can fit our model using `fit()` along with the training dataset.

```{r}
lend_linear_fit <-
  lend_linear_wflow |>
  fit(data = lend_train)
```

Below are our predicted lending rates:

```{r}
#| warning: false
predict(lend_linear_fit, lend_test)
```

Now, let's evaluate our performance. We can analyze our results by calculating the coefficient of determination, which is 0.6 in this case. While this indicates some level of explanatory power, it suggests that our model's estimate may not be very robust.

```{r}
#| warning: false
lend_linear_results <-
  bind_cols(predict(lend_linear_fit, lend_train)) |>
  bind_cols(lend_train |>
              select(int_rate))

rsq(lend_linear_results, truth = int_rate, estimate = .pred)
```

However, one of the advantages of tidymodels is the ability to quickly pivot with the steps we've already executed. Instead of a linear model, we can now define a random forest model:

```{r}
lend_rand <- rand_forest(mode = "regression") |>
  set_engine("ranger",
             importance = "permutation")
```

Then, we can pass that model into our workflow and proceed to fit the model:

```{r}
#| cache: true
lend_ranger_wflow <- 
  workflow() |>
  add_model(lend_rand) |>
  add_recipe(rec_obj)

lend_ranger_fit <-
  lend_ranger_wflow |>
  fit(data = lend_train)
```

Now, let's recalculate the coefficient of determination:

```{r}
lend_ranger_results <-
  bind_cols(predict(lend_ranger_fit, lend_train)) |>
  bind_cols(lend_train |>
              select(int_rate))

rsq(lend_ranger_results, truth = int_rate, estimate = .pred)
```

Now, that's a significant improvement!

Given the number of variables in our model, it's natural to want to identify the most important ones. We can compute variable importance by running `vi` to achieve this.

```{r}
vip:::vi(lend_ranger_fit) |>
  arrange(desc(Importance))
```

According to this analysis, the most important variables are "term", "installment_pct_inc", "bc_open_to_buy", "installment", and "loan_to_income".

# Model logging and artifact storage

If we aim to log our model for use in other contexts, like a Shiny app, we can utilize the [vetiver package]. This tool facilitates deploying and maintaining machine learning models in production, allowing us to store models in a pin for convenient access and reference.

Let's reconstruct our model using only the top five variables of importance:

```{r}
#| cache: true
imp_var <- c("term", "installment_pct_inc", "bc_open_to_buy", "installment", "loan_to_income")

lendingclub_dat_cols_select <- 
  lendingclub_dat_cols |> 
  select(int_rate, all_of(imp_var))

train_test_split_select <- initial_split(lendingclub_dat_cols_select)

lend_train_select <- training(train_test_split_select)
lend_test_select <- testing(train_test_split_select)

rec_obj_select <- recipe(int_rate ~ ., data = lend_train_select) |>
  step_normalize(c("installment", "installment_pct_inc", "loan_to_income")) |>
  step_impute_mean("bc_open_to_buy") |> 
  step_other()

lend_wflow_select <- 
  workflow() |>
  add_model(lend_rand) |>
  add_recipe(rec_obj_select)

lend_fit_select <-
  lend_wflow_select |>
  fit(data = lend_train_select)
```

We can create a vetiver object `v` to store the trained model:

```{r}
#| eval: false
v <- vetiver_model(lend_fit_select, "lend_fit")
```

We can deploy the model by creating a special Plumber router in R with the plumber package. We add a POST endpoint for making predictions. Following that, we connect to the destination where we intend to store our model artifact. In our case, this would be Posit Connect. Then, we save the vetiver model to a pin for future access.

```{r}
#| eval: false
board <-
  pins::board_connect(auth = "manual",
                      server = Sys.getenv("CONNECT_SERVER"),
                      key = Sys.getenv("CONNECT_API_KEY"))

board %>% vetiver_pin_write(v)
```

Now, we can deploy the model to Posit Connect to generate predictions as needed:

```{r}
#| eval: false
vetiver_deploy_rsconnect(
  board = board,
  name = "isabella.velasquez/lend_fit",
  predict_args = list(debug = TRUE)
)
```

Now that our model is available through an API, we can use it in other places -- such as a Shiny app!

# Interactive Shiny app

Since our model is pinned in an API, it can be retrieved by other applications, such as a Shiny app. Within the `server` section of our Shiny app, we retrieve the data by connecting to our pin board:

```{r}
#| eval: false
board <- pins::board_connect(server = Sys.getenv("CONNECT_SERVER"),
                             key = Sys.getenv("CONNECT_API_KEY"))
```

Access the endpoint URL:

```{r}
#| eval: false
url <-
  "https://pub.palm.ptd.posit.it/content/50a4870a-92da-41ec-81cc-8240ae70c9f6/predict"

endpoint <- vetiver_endpoint(url)
```

Also in the server portion of the Shiny app, establish a reactive variable (named `predictions_df` here) that generates a tibble from the user's inputs and passes it to the endpoint.

```{r}
#| eval: false
predictions_df <- reactive({
  req(
    input$select_term,
    input$input_installment_pct_inc,
    input$input_bc_open_to_buy,
    input$input_installment,
    input$input_loan_to_income
  )
  
  pred_tibble <-
    tibble(
      term = input$select_term,
      installment_pct_inc = input$input_installment_pct_inc,
      bc_open_to_buy = input$input_bc_open_to_buy,
      installment = input$input_installment,
      loan_to_income = input$input_loan_to_income
    )
  
  url <-
    "https://pub.palm.ptd.posit.it/content/50a4870a-92da-41ec-81cc-8240ae70c9f6/predict"
  
  endpoint <- vetiver_endpoint(url)
  
  apiKey = Sys.getenv("CONNECT_API_KEY")
  
  predictions <-
    predict(endpoint,
            pred_tibble,
            httr::add_headers(Authorization = paste("Key", apiKey)))
  
})
```

Below is our Shiny app for predicting lending rates. Each time you select an input, it calculates the rate based on our random forest model. This app is embedded in a [Quarto document](https://quarto.org/) using [ShinyLive](https://posit-dev.github.io/r-shinylive/) and the [Quarto ShinyLive extension](https://github.com/coatless-quarto/r-shinylive-demo). The design and layout of the Shiny app is developed with [bslib](https://rstudio.github.io/bslib/).

In summary, we've gone on a comprehensive journey to predict lending rates using machine learning techniques within the context of financial analysis. We began by accessing our data from Databricks, cleaning our data, and using tidymodels for modeling and evaluation. We developed two predictive models, evaluated their performance, and identified key variables driving interest rate predictions.

Leveraging the power of vetiver, we deployed our model for seamless integration into production environments, such as Shiny apps. This holistic approach can help enhance decision-making processes within the financial domain.

:::{.column-page}

```{shinylive-r}
#| standalone: true
#| eval: false
#| echo: false
#| viewerHeight: 400
library(shiny)
library(bslib)
library(bsicons)
library(dplyr)
library(vetiver)
tidymodels::tidymodels_prefer(quiet = TRUE)

cards <- list(
  card(full_screen = TRUE,
       card_header("Term of loan:"),
       card_body(
         selectInput(
           inputId = "select_term",
           choices = c("36 months", "60 months"),
           selected = "36 months",
           label = "Select 36 or 60 months"
         )
       )), 
  card(full_screen = TRUE,
       card_header("Proportion of monthly income the installment payment represents:"),
       card_body(
         numericInput(
           inputId = "input_installment_pct_inc",
           label = "Input a number between 0 and 1:",
           value = 0.07,
           min = 0,
           max = 1,
           step = 0.01
         )
       )),
  card(full_screen = TRUE,
       card_header("Open to buy on revolving bankcards"),
       card_body(
         numericInput(
           inputId = "input_bc_open_to_buy",
           label = "Input a number between 0 and 500,000:",
           value = 14467,
           min = 0,
           max = 500000,
           step = 1000
         )
       )),
  card(full_screen = TRUE,
       card_header("Installment payment"),
       card_body(
         numericInput(
           inputId = "input_installment",
           label = "Input a number between 30 and 2000:",
           value = 463,
           min = 30,
           max = 2000,
           step = 100
         )
       )),
  card(full_screen = TRUE,
       card_header("Loan to income ratio"),
       card_body(
         numericInput(
           inputId = "input_loan_to_income",
           label = "Input a number between 0 to 2",
           value = 0.21,
           min = 0,
           max = 2,
           step = 0.1
         )
       ))
)

vbs <- list(
  value_box(
    title = "Predicted lending rate",
    value = textOutput("pred_int"),
    style = "background-color: #082D46!important; color: #FFFFFF!important",
    showcase = bsicons::bs_icon("bank", size = "0.75em"),
    showcase_layout = "top right",
    full_screen = FALSE,
    fill = TRUE,
    height = NULL
  )
)

foot <-
  tags$div(
    style = "background-color: #FFFFFF; padding: 0px; text-align: center; bottom: 0; width: 100%;",
    HTML(
      "Powered by <a href='https://posit.co'><img src='https://www.rstudio.com/assets/img/posit-logo-fullcolor-TM.svg' alt='Posit Logo' style='width:55px;'></a> | Integrated with <a href='https://www.databricks.com'><img src='https://cdn.cookielaw.org/logos/29b588c5-ce77-40e2-8f89-41c4fa03c155/bc546ffe-d1b7-43af-9c0b-9fcf4b9f6e58/1e538bec-8640-4ae9-a0ca-44240b0c1a20/databricks-logo.png' alt='Databricks Logo' style='width:85px;'></a>. For more details, see our <a href='https://posit.co/blog/databricks-and-posit-announce-new-integrations/' target='_blank'>blog post</a> announcing the partnership."
    )
  )

ui <- bslib::page(
  title = "lending rate prediction app",
  layout_columns(width = 1/5,
                 height = 200,
                 cards[[1]], cards[[2]], cards[[3]], cards[[4]], cards[[5]]),
  layout_columns(vbs[[1]]),
  card_footer(foot)
)

server <- function(input, output, session) {
  
  board <- pins::board_connect(server = Sys.getenv("CONNECT_SERVER"),
                               key = Sys.getenv("CONNECT_API_KEY"))
  
  predictions_df <- reactive({
    
    req(input$select_term, input$input_installment_pct_inc, input$input_bc_open_to_buy, input$input_installment, input$input_loan_to_income)
    
    pred_tibble <-
      tibble(
        term = input$select_term,
        installment_pct_inc = input$input_installment_pct_inc,
        bc_open_to_buy = input$input_bc_open_to_buy,
        installment = input$input_installment,
        loan_to_income = input$input_loan_to_income
      )
    
    url <-
      "https://pub.palm.ptd.posit.it/content/50a4870a-92da-41ec-81cc-8240ae70c9f6/predict"
    
    endpoint <- vetiver_endpoint(url)
    
    apiKey = Sys.getenv("CONNECT_API_KEY")
    
    predictions <-
      predict(endpoint,
              pred_tibble,
              httr::add_headers(Authorization = paste("Key", apiKey)))
    
  })
  
  output$pred_int <- renderText({
    predictions_df()$.pred
  })
  
}

shinyApp(ui, server)
```

:::