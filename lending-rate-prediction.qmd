---
title: "Predicting lending rates with Databricks and tidymodels"
format:
  html:
    css: styles.css
    code-fold: true
    code-summary: Show the code
    code-links:
      - text: "GitHub repository"
        icon: github
        target: blank
        href: https://github.com/posit-dev/databricks-finance-app
      - text: "Shiny app"
        icon: window
        target: blank
        href: https://pub.demo.posit.team/public/predicted-interest-rate-calculator/
title-block-banner: banner.png
toc: true
filters:
  - shinylive
resource_files:
  - banner.png
---

Machine learning algorithms are reshaping financial decision-making, changing how the industry manages financial risk. These advanced algorithms can analyze vast amounts of data to deliver predictive insights, which creates new opportunities to drive informed decisions or to expedite client service. 

And now, machine learning is more streamlined than it has ever been. Financial analysts [can use Posit](https://posit.co/blog/databricks-and-posit-announce-new-integrations/) to harness the performance and data governance capabilities of Databricks. Analysts can use:

* [ODBC](https://odbc.r-dbi.org/) to link with a Databricks warehouse 
* [sparklyr](https://spark.posit.co/deployment/databricks-connect.html) to interface with a Databricks cluster

Moreover, they can work in managed environments with their preferred IDE in [Posit Workbench](https://posit.co/products/enterprise/workbench/) and deploy and share their models on [Posit Connect](https://posit.co/products/enterprise/connect/). This workflow combines the power of R's [tidymodels framework for machine learning](https://www.tidymodels.org/) with the storage and computation capabilities of Databricks.

In this article, we will use both Posit and Databricks to apply machine learning methods to the consumer credit market, where accurately predicting lending rates is critical for customer acquisition. Credit lenders must use a comprehensive process, transparent to regulators, to choose lending rates for loan applicants---but this takes time. Customers shopping between lenders want personalized lending rates, quickly. To avoid losing customers to faster alternatives, while still minimizing loan associated risks, we will build a user friendly Shiny app, based on a machine learning model, that customers can use to predict the interest rates on their loans _before_ they submit their applications.

Specifically, we will:

1. Connect to historical lending rate data stored in Databricks Delta Lake
1. Use tidymodels to tune and cross-validation a penalized linear regression (LASSO) that predicts interest rates
1. Use the penalized linear regression model (LASSO) to select a small set of variables for users to input into our model
1. Build an interactive Shiny app to provide a customer-facing user interface for our model
1. Deploy the app to production on Posit Connect

## Connect to Databricks Delta Lake from RStudio IDE

For those with access to Databricks, add the historical lending rate data to your catalog by running the below:

```sql
CREATE TABLE lending_club USING com.databricks.spark.csv OPTIONS(path 'dbfs:/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv', header "true");
```

## Working with Databricks tables

Let's start by loading the packages that will be integral to our workflow.

```{r}
#| label: setup
#| message: false
# Importing data
library(DBI)
library(odbc)

# Wrangling packages
library(tidyverse)
library(dbplyr)

# Visualization packages
library(dbplot)
library(ggplot2)

# Modeling packages
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(yardstick)
library(glmnet)
library(dials)
library(tune)
library(broom)
```

As previously mentioned, analysts have several options for accessing Databricks data. In this walk-through, we'll demonstrate connectivity using the [odbc package](https://odbc.r-dbi.org/). The Posit Solutions site provides detailed guidance on using the new [odbc `databricks()` function](https://solutions.posit.co/connections/db/databases/databricks/index.html#using-the-new-odbcdatabricks-function). Essentially, we need to store a Databricks token and host URL in our R environment. The function itself requires only the HTTP path to the SQL warehouse.

```{r}
#| label: odbc-connection
# Sys.setenv("DATABRICKS_HOST" = "your-databricks-host.com")
# Sys.setenv("DATABRICKS_TOKEN" = "your-databricks-token")
con <-
  dbConnect(odbc::databricks(), httpPath = Sys.getenv("HTTP_PATH"))
```

This code snippet establishes a connection using the `con` object. With this connection, we can create an object named `lendingclub_dat` that refers to our table in Databricks. 

```{r}
#| label: get-tbl-data
lendingclub_dat <-
  tbl(con,
      dbplyr::in_catalog("hive_metastore", "default", "lendingclub"))
```

`tbl()` creates the R equivalent of an SQL view: an SQL query that represents a table. For example, `lendingclub_dat` is the virtual table represented by this SQL query. 

```{r}
lendingclub_dat |> 
  show_query()
```

As a result, `lendingclub_dat` is very lightweight---it has no data. The data still lives in Databricks. 

But we can still work with `lendingclub_dat` as if it were a data frame. When we manipulate a tbl with dplyr or base R code, dbplyr translates our code to SQL and adds it to the query. 

```{r}
lendingclub_dat |> 
  head() |> 
  select(int_rate, bc_util) |> 
  show_query()
```

When we wish to run the accumulated query and access the result, we can call `collect()`. R will send the SQL query to Databricks to be evaluated _in Databricks' compute_. It will then import the result as a real table into R, where we can continue to manipulate it in R, perhaps to run advanced operations on the data that have no equivalent in SQL. 

```{r}
lendingclub_dat |> 
  head() |> 
  select(int_rate, bc_util) |> 
  collect()
```

R will also lazily call `collect()` when it needs to to show us a result we ask for. 

```{r}
lendingclub_dat |> 
  head() |> 
  select(int_rate, bc_util)
```

Since computations normally run faster in Databricks than in R, running operations before pulling data into R with `collect()` leads to significantly faster performance.



We can manipulate `lendingclub_dat` with dplyr code as if it were a table, but it is not a table and we did not transfer any data from Databricks into R by creating `lendingclub_dat`. 


```{r}
lendingclub_dat |> show_query()
```



We can use the [dbplot package](https://edgararuiz.github.io/dbplot/) to generate a ggplot without transferring the data into R. This visualization displays the distribution of the variable we aim to predict, lending rate. Loan amounts are plotted on the x-axis, while the frequency of loans at each amount is depicted on the y-axis. It's shown that the majority of loans have an lending rate of less than 20%.

```{r}
#| label: dbplot-int-rate
#| fig-asp: 0.618
#| warning: false
lendingclub_dat |>
  mutate(int_rate = as.numeric(REGEXP_REPLACE(int_rate, "%", ""))) |>
  db_compute_bins(int_rate, binwidth = 0.5) |>
  ggplot() +
  geom_col(
    aes(x = int_rate, y = count),
    fill = "#4682b4",
    color = "#4682b4",
    alpha = 0.4
  ) +
  labs(
    title = "Distribution of lending rate",
    x = "Lending rate",
    y = "Count"
  ) +
  theme_minimal()
```

## Data cleaning and feature engineering

The `lendingclub_dat` object resembles a typical data frame, but it's actually a SQL table. With the [dbplyr package](https://dbplyr.tidyverse.org/), we can use dplyr syntax to query and manipulate these SQL tables directly in R.

Certain columns in `lendingclub_dat` are imported as characters instead of their intended numeric type. The code below shows how we can use dplyr functions to convert them to the correct type. dbplyr then converts this dplyr code into SQL. 

You can see the underlying SQL query by running `show_query()`. Click on the `SQL syntax` tab to see the results of `show_query()`. This example highlights the efficiency of dplyrâ€™s syntax in R, offering a more concise alternative to verbose SQL queries.

::: {.panel-tabset}

## R syntax

```{r}
#| label: dplyr-syntax
#| code-fold: false
#| eval: false
tbl(con,
    dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |>
    mutate(across(c(starts_with("annual")), ~ as.numeric(.)))
```

## SQL syntax

```{r}
#| label: sql-syntax
#| code-fold: false
tbl(con,
    dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |>
    mutate(across(c(starts_with("annual")), ~ as.numeric(.))) |>
    show_query()
```

:::

Now, we can proceed with using dplyr to clean up the rest of the variables and create supplementary variables that could potentially influence lending rates. Running operations before pulling the data into R with `collect()` leads to significantly faster performance.

```{r}
#| label: pre-collect-data-clean
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |> 
  select(-c("acc_now_delinq", "chargeoff_within_12_mths",
            "debt_settlement_flag", "debt_settlement_flag_date",
            "deferral_term",
            "delinq_amnt","desc","disbursement_method","emp_title",
            "funded_amnt","funded_amnt_inv","grade","hardship_amount",
            "hardship_dpd", "hardship_end_date", "hardship_flag",
            "hardship_last_payment_amount", "hardship_length",
            "hardship_loan_status", "hardship_payoff_balance_amount",
            "hardship_reason", "hardship_start_date", "hardship_status",
            "last_credit_pull_d",
            "hardship_type","id","initial_list_status","installment","issue_d",
            "last_pymnt_d", "last_pymnt_amnt", "loan_status",
            "member_id", "next_pymnt_d", "num_tl_30dpd", "num_tl_120dpd_2m", 
            "orig_projected_additional_accrued_interest",
            "out_prncp", "out_prncp_inv","payment_plan_start_date",
            "policy_code","purpose", "pymnt_plan", "revol_bal_joint",
            "revol_util", "sec_app_earliest_cr_line",
            "sec_app_inq_last_6mths", "sec_app_mort_acc", "sec_app_open_acc",
            "sec_app_revol_util", "sec_app_open_act_il",
            "sec_app_num_rev_accts", "sec_app_chargeoff_within_12_mths",
            "sec_app_collections_12_mths_ex_med",
            "sec_app_mths_since_last_major_derog","settlement_amount",
            "settlement_date", "settlement_percentage", "settlement_status",
            "settlement_term","sub_grade","title", "total_pymnt", "total_pymnt_inv",
            "total_rec_int", "total_rec_late_fee", "total_rec_prncp", # "total_rev_hi_lim",
            "url","verification_status",
            "verification_status_joint")) |>
  mutate(
    # Convert these columns into numeric
    across(c(starts_with("annual"), starts_with("dti"), starts_with("inq"),  
             starts_with("mo"), starts_with("mths"), starts_with("num"), 
             starts_with("open"), starts_with("percent"), starts_with("pct"), 
             starts_with("revol"), starts_with("tot"),  "acc_open_past_24mths", 
             "all_util", "avg_cur_bal","bc_open_to_buy", "bc_util", 
             "collections_12_mths_ex_med", "collection_recovery_fee", "delinq_2yrs", 
             "il_util", "loan_amnt", "max_bal_bc", "pub_rec", 
             "pub_rec_bankruptcies", "recoveries", "tax_liens"), 
           ~ as.numeric(.)),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate the percentage of the borrower's total income that current debt 
    # obligations, including this loan, will represent
    adjusted_dti = case_when(
      application_type == "Individual" ~ (loan_amnt + tot_cur_bal) / (annual_inc),
      .default = (loan_amnt + tot_cur_bal) / (annual_inc_joint)
    ),
    #  Calculate utilization on installment accounts excluding mortgage balance
    il_util_ex_mort = case_when(
      total_il_high_credit_limit > 0 ~ total_bal_ex_mort / total_il_high_credit_limit,
      .default = 0
    ),
    # Fill debt to income joint with individual debt to income where missing
    dti_joint = coalesce(dti_joint, dti),
    # Fill annual income joint with individual annual income where missing
    annual_inc_joint = coalesce(annual_inc_joint, annual_inc)) |> 
  collect()
```

After running `collect()`, when we have our data frame in R, we gain access to additional advanced R functions that are not available in SQL. This enables us to further refine and clean our data as needed.

```{r}
#| label: post-collect-data-clean
lendingclub_dat_clean <-
  lendingclub_dat |>
  mutate(
    # Missing values for these columns seem most appropriate to fill with zero
    across(c("inq_fi", "dti", "all_util", "percent_bc_gt_75", "il_util", 
             "avg_cur_bal","all_util", "il_util", "inq_last_6mths", "inq_last_12m", 
             "open_il_12m", "open_il_24m", "open_rv_12m", "open_rv_24m"), 
           ~ replace_na(., 0)),
    # Missing values for these columns seem most appropriate to fill with the column max
    across(c("mo_sin_old_il_acct", "mths_since_last_major_derog", "mths_since_last_delinq", 
             "mths_since_recent_bc", "mths_since_last_record", "mths_since_rcnt_il", 
             "mths_since_recent_bc", "mths_since_recent_bc_dlq", "mths_since_recent_inq", 
             "mths_since_recent_revol_delinq", "mths_since_recent_revol_delinq"),  
           ~ replace_na(., max(., na.rm = TRUE))),
    # Remove percent sign
    int_rate = as.numeric(stringr::str_remove(int_rate, "%")),
    # Create variable for earliest line of credit
    earliest_cr_line = lubridate::parse_date_time2(paste("01", earliest_cr_line, sep = "-"), 
                                                   "dmy", cutoff_2000 = 50L),
    # Calculate time since earliest line of credit
    age_earliest_cr = lubridate::interval(as.Date(earliest_cr_line), 
                                          as.Date(lubridate::today())) %/% lubridate::days(1),
    # Convert characters to factors
    across(where(is.character), .fns = as.factor),
    # Encode ordered factors
    term = as.numeric(stringr::str_trim(stringr::str_remove(term, "months"))),
    emp_length = as.ordered(factor(emp_length, 
                                   levels = c("< 1 year", "1 year", "2 years", 
                                              "3 years", "4 years", "5 years",
                                              "6 years", "7 years", "8 years", 
                                              "9 years", "10+ years")))) |> 
  # drop date column
  select(!earliest_cr_line) |> 
  filter(!is.na(int_rate))
```

With our data cleaned up, it's time to examine the variables we intend to include in the model. We create vectors for conveniently referencing variable categories later if necessary. Then, we compile all the desired columns into one vector named `all_vars` for use in our model.

```{r}
#| label: create-variable-vectors
mean_impute_vals <- 
  c("bc_util", "num_rev_accts", "bc_open_to_buy", "emp_length", "percent_bc_gt_75", 
    "total_bal_il", "total_il_high_credit_limit", "total_cu_tl")

categorical_vars <- 
  c("addr_state", "application_type", "home_ownership", "emp_length", "term", 
    "zip_code")
```

To ensure our dataset is free of any missing values, we can run the below:

```{r}
#| label: check-nas
#| code-fold: false
colSums(is.na(lendingclub_dat_clean))
sum(colSums(is.na(lendingclub_dat_clean)) > 0)
```

We can ensure that no factor columns have fewer than two factors by running the below:

```{r}
#| label: check-factors
#| code-fold: false
lendingclub_dat_clean |>
  select(where(is.factor)) %>%
  select(where( ~ nlevels(.) < 2))
```

## Variable selection

We will use a lasso regression to choose a succinct, but efficient set of variables to build our model around.

## Model creation

With our dataset prepared, we can begin the modeling process. This involves creating train and test datasets, which we'll use to train and evaluate our model.

```{r}
#| label: create-test-train
set.seed(1234)

train_test_split <- initial_split(lendingclub_dat_clean)
lend_train <- training(train_test_split)
lend_test <- testing(train_test_split)
```

Using tidymodels, we can construct a "recipe" detailing the decisions we want to make for our data:

* `step_normalize()`: This recipe step normalizes numeric data to have a standard deviation of one and a mean of zero. Given that our dataset contains numeric values of various units (e.g., dollars, months), normalization ensures that variables with larger value ranges do not disproportionately influence the model.
* `step_impute_mean()`: This recipe step replaces missing values of numeric variables with the mean of those values in the training set. Performing this step now, rather than during the data cleaning phase, prevents information leakage from the testing set into the model. By calculating the mean solely using the training set, we avoid bias when applying it to the testing data during model evaluation.

```{r}
#| label: create-recipe
rec_obj <- recipe(int_rate ~ ., data = lend_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_ordinalscore(emp_length) |> 
  step_integer(c("addr_state", "application_type", "home_ownership", "zip_code")) |> 
  step_impute_mean(all_of(mean_impute_vals))
```

To verify that the recipe functions as intended, we can run `prep(rec_obj, lend_train) |> bake(newdata = NULL)`. This will display the data that the workflow will provide to the model.

```{r}
#| label: display-workflow-data
#| warning: false
#| message: false
prep(rec_obj, lend_train) |>  bake(new_data = NULL)
```

Now, we proceed with a lasso model. `linear_reg()` specifies a model capable of predicting numeric values from predictors using a linear function. We construct a workflow incorporating our recipe alongside the linear model.

```{r}
#| label: run-lasso-workflow
lend_lasso <- 
  linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")

lend_lasso_wflow <-
  workflow() |>
  add_model(lend_lasso) |>
  add_recipe(rec_obj)
```

# Parameter tuning

We can find the optimal value of lambda with cross-validation.

```{r}
#| label: tune-penalty-parameter
lambda_grid <- 
  grid_regular(penalty(), levels = 50)

lasso_grid <- 
  lend_lasso_wflow |> 
  tune_grid(grid = lambda_grid, resamples = vfold_cv(lend_train))
```

We can explore our results as a table.

```{r}
#| label: lambda-metrics
lasso_grid |> 
  collect_metrics()
```

Or more commonly, as a lasso path plot.

```{r}
#| label: lambda-plot
lasso_grid |> 
  autoplot()
```

Every parameter below 0.01 seems similar, but 0.01 will give us a parsimonious model. We add the parameter value we choose to our workflow.

```{r}
#| label: finalize-lambda
final_lasso_wflow <- 
  lend_lasso_wflow |> 
  finalize_workflow(list(penalty = 0.1))
```

How well does that model do?. Now, we can fit our model using `fit()` along with the training dataset.

```{r}
#| label: fit-lasso-workflow
lend_lasso_fit <-
  final_lasso_wflow |>
  fit(data = lend_train)
```

Below are our predicted lending rates:

```{r}
#| label: predict-lasso-workflow
#| warning: false
predict(lend_lasso_fit, new_data = lend_train) 
```

Now, let's evaluate our performance. We can analyze our results by calculating the coefficient of determination, which is 0.6 in this case. While this indicates some level of explanatory power, it suggests that our model's estimate may not be very robust.

```{r}
#| label: rsq-lasso-workflow
#| warning: false
lend_lasso_results <-
  bind_cols(predict(lend_lasso_fit, lend_train)) |>
  bind_cols(lend_train |> select(int_rate))

rsq(lend_lasso_results, truth = int_rate, estimate = .pred)
rmse(lend_lasso_results, truth = int_rate, estimate = .pred)
```

# A linear model

# How many variables are in this model?

# library(broom)

lend_lasso_fit |>
  extract_fit_parsnip() |> 
  tidy() |> 
  filter(estimate > 0) |> 
  nrow()
# 20


# Variable Selection
# ------------------------------------------------------------------------------
# Unfortunately, we think that clients who use our app will only have patience
# to input five variables. Hence we need to find the top 5.
lend_lasso_fit |> 
  extract_fit_parsnip() |> 
  autoplot()

lend_lasso_fit |> 
  extract_fit_parsnip() |> 
  autoplot(min_penalty = 1, top_n = 5)


# Sacrifice in performance
# ------------------------------------------------------------------------------
# How much performance do we sacrifice by only using four variables?

set.seed(1234)
lendingclub_dat_reduced <-
  lendingclub_dat_clean |> 
  select(int_rate, term, bc_open_to_buy, bc_util, all_util)

reduced_split <- initial_split(lendingclub_dat_reduced)

reduced_train <- training(reduced_split)
reduced_test <- testing(reduced_split)

red_rec_obj <- recipe(int_rate ~ ., data = reduced_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_impute_mean(all_of(c("bc_open_to_buy", "bc_util")))

lend_linear <- 
  linear_reg()

lend_linear_wflow <-
  workflow() |>
  add_model(lend_linear) |>
  add_recipe(red_rec_obj)

lend_linear_fit <-
  lend_linear_wflow |>
  fit(data = reduced_train)

lend_linear_results <-
  bind_cols(predict(lend_linear_fit, reduced_train)) |>
  bind_cols(reduced_train |> select(int_rate))

rsq(lend_linear_results, truth = int_rate, estimate = .pred)
rmse(lend_linear_results, truth = int_rate, estimate = .pred)

# We will deal


# Deploy Model
# ------------------------------------------------------------------------------

library(readr)

lend_linear_fit |> 
  write_rds(file = "app/model.RDS")



# Interactive Shiny app

Since our model is pinned in an API, it can be retrieved by other applications, such as a Shiny app. Within the `server` section of our Shiny app, we retrieve the data by connecting to our pin board:

```{r}
#| label: connect-shiny-app
#| eval: false
#| code-fold: false
board <-
  board_connect(server = Sys.getenv("CONNECT_SERVER"),
                key = Sys.getenv("CONNECT_API_KEY"))
```

Access the endpoint URL with `vetiver_endpoint()`:

```{r}
#| label: access-shiny-endpoint
#| code-fold: false
#| eval: false
url <- "https://pub.demo.posit.team/public/lend-fit-vetiver-api/"

endpoint <- vetiver_endpoint(url)
```

Also in the server portion of the Shiny app, establish a reactive variable (named `predictions_df` here) that generates a tibble from the user's inputs and passes it to the endpoint.

```{r}
#| label: make-shiny-predictions
#| eval: false
#| code-fold: false
predictions_df <- reactive({
  req(
    input$select_term,
    input$input_installment_pct_inc,
    input$input_bc_open_to_buy,
    input$input_installment,
    input$percent_bc_gt_75
  )

  pred_tibble <-
    tibble(
      term = input$select_term,
      installment_pct_inc = input$input_installment_pct_inc,
      bc_open_to_buy = input$input_bc_open_to_buy,
      installment = input$input_installment,
      percent_bc_gt_75 = input$percent_bc_gt_75
    )

  url <- "https://pub.demo.posit.team/public/lend-fit-vetiver-api/predict"

  endpoint <- vetiver_endpoint(url)

  api_key <- Sys.getenv("CONNECT_API_KEY")

  predictions <- predict(
    endpoint,
    pred_tibble,
    httr::add_headers(Authorization = paste("Key", api_key))
  )
})
```

Below is our Shiny app for predicting lending rates. Each time you select an input, it calculates the rate based on our random forest model. The design and layout of the Shiny app is developed with [bslib](https://rstudio.github.io/bslib/).

::: {.panel-tabset}

## app.R

```{r}
#| label: app.R
#| eval: false
# install.packages("remotes")
# remotes::install_github("rstudio/shiny") # for version 1.8.1.9001
library(shiny)
library(tibble)
library(readr)
library(workflows)
library(recipes)

source("authenticate.R")
source("cards.R")
source("helpers.R")

rates <- connect_to_lending_club_data_on_databricks()
endpoint <- read_rds("model.RDS")

ui <- bslib::page_navbar(
  title = "Predicted Interest Rate Calculator",
  theme = bs_theme(bootswatch = "flatly", success = "#4682b4"),
  bg = "#082D46",
  underline = FALSE,
  nav_panel(
    title = " ",
    layout_columns(
      
      # Row 1
      card(
        helpText("Please fill out the fields below. Then click Predict Rate."),
        layout_columns(
          cards[[1]], 
          cards[[2]], 
          cards[[3]], 
          cards[[4]], 
          cards[[5]],
          width = 1/5, 
          height = 170
        ),
        actionButton(
          "predict", "Predict Rate", width = 200, 
           style="color: #fff; background-color: #4682b4;"
        )
      ),
      
      # Row 2
      layout_columns(
        conditionalPanel(condition = "input.predict > 0", vbs),
        conditionalPanel(condition = "input.predict > 0", plot),
        height = 500, col_widths = c(3, 9)
      ),
      col_widths = c(12, 12)
    ),
    card_footer(foot)
  )
)

server <- function(input, output, session) {
  
  all_util <- 
    reactive(input$all_balance / input$all_limit)
  
  bc_util <- 
    reactive(input$bc_balance / input$bc_limit)
  
  bc_open_to_buy <-
    reactive(input$bc_limit - input$bc_balance)
  
  predictions_df <- 
    reactive({
      
      pred_tibble <-
        tibble(term = as.numeric(input$term),
               all_util = all_util(),
               bc_util = bc_util(),
               bc_open_to_buy = bc_open_to_buy())
      
      predict(endpoint, new_data = pred_tibble)
    }) |> 
    bindCache(input$term, 
              input$all_balance, 
              input$all_limit, 
              input$bc_balance, 
              input$bc_limit) |> 
    bindEvent(input$predict)
  
  output$pred_int <- 
    renderText({
      predictions_df()$.pred |> round(2)
    })
  
  rate_distribution <- 
    reactive({
      rates |> 
        find_rates_for_similar_applicants(input$term, 
                                          all_util(), 
                                          bc_util(), 
                                          bc_open_to_buy())
    })  |> 
    bindCache(input$term, 
              input$all_balance, 
              input$all_limit, 
              input$bc_balance, 
              input$bc_limit) |> 
    bindEvent(input$predict)
  
  output$plot <-
    renderPlot({
      plot_rate_distribution(rate_distribution())
    })
  
}

shinyApp(ui, server)
```

## authenticate.R

```{r}
#| label: authenticate.R
#| eval: false
# authenticate.R
library(dplyr)
library(dbplyr)
library(odbc)

# Sys.setenv("DATABRICKS_HOST" = "your-databricks-host.com")
# Sys.setenv("DATABRICKS_TOKEN" = "your-databricks-token")
con <-
  dbConnect(odbc::databricks(), httpPath = Sys.getenv("HTTP_PATH"))

# Return connection to lending club table
connect_to_lending_club_data_on_databricks <- function(){
  tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |>
    select(int_rate, term, all_util, bc_util, bc_open_to_buy) |> 
    filter(!is.na(int_rate)) |> 
    mutate(int_rate = REPLACE(int_rate, "%", ""),
           term = SUBSTRING(term, 2,2),
           across(everything(), ~ as.numeric(.)))
}
```

## cards.R

```{r}
#| label: cards.R
#| eval: false
# cards.R

# install.packages("remotes")
# remotes::install_github("rstudio/bslib") # for version 0.7.1
library(bslib)
library(bsicons)

cards <- list(
  card(
    card_body(
      selectInput(
        inputId = "term",
        choices = list("36 months" = 36, "60 months" = 60),
        selected = 36,
        label = tooltip(
          trigger = list("Term of loan", bs_icon("info-circle")),
          "How soon would you like to pay off the loan?"
        )
      )
    )
  ),
  
  card(
    card_body(
      numericInput(
        inputId = "all_balance",
        value = 5000,
        min = 0,
        max = 500000,
        step = 1000,
        label = tooltip(
          trigger = list("Credit Balance", bs_icon("info-circle")),
          "How much credit, in dollars, do you currently have withdrawn from all sources of credit, including bank cards?"
        )
      )
    )
  ),
  
  card(
    card_body(
      numericInput(
        inputId = "all_limit",
        value = 10000,
        min = 0,
        max = 500000,
        step = 1000,
        label = tooltip(
          trigger = list("Credit Limit", bs_icon("info-circle")),
          "What is your total credit limit, in dollars, for all sources of credit, including bank cards?"
        )
      )
    )
  ),
  
  card(
    card_body(
      numericInput(
        inputId = "bc_balance",
        value = 5000,
        min = 0,
        max = 500000,
        step = 1000,
        label = tooltip(
          trigger = list("Bank Card Balance", bs_icon("info-circle")),
          "How much credit, in dollars, do you currently have withdrawn from only bank cards?"
        )
      )
    )
  ),
  
  card(
    card_body(
      numericInput(
        inputId = "bc_limit",
        value = 10000,
        min = 0,
        max = 500000,
        step = 1000,
        label = tooltip(
          trigger = list("Bank Card Limit", bs_icon("info-circle")),
          "What is your total credit limit, in dollars, for only bank cards?"
        )
      )
    )
  )
)

vbs <-
  value_box(
    title = "Predicted interest rate",
    value = textOutput("pred_int"),
    style = "background-color: #082D46!important; color: #FFFFFF!important",
    showcase = bsicons::bs_icon("bank", fill = '#4682b4 !important;'),
    showcase_layout = "top right",
    height = 500
  )

foot <-
  tags$div(
    style = "background-color: #FFFFFF; padding: 0px; text-align: center; bottom: 0; width: 100%;",
    HTML(
      "Powered by <a href='https://posit.co'><img src='https://www.rstudio.com/assets/img/posit-logo-fullcolor-TM.svg' alt='Posit Logo' style='width:55px;'></a> | Integrated with <a href='https://www.databricks.com'><img src='https://cdn.cookielaw.org/logos/29b588c5-ce77-40e2-8f89-41c4fa03c155/bc546ffe-d1b7-43af-9c0b-9fcf4b9f6e58/1e538bec-8640-4ae9-a0ca-44240b0c1a20/databricks-logo.png' alt='Databricks Logo' style='width:85px;'></a>. For more details, see our <a href='https://posit.co/blog/databricks-and-posit-announce-new-integrations/' target='_blank'>blog post</a> announcing the partnership."
    )
  )

plot <-
  card(full_screen = TRUE,
       card_header(HTML("Applicants like you have received these interest rates")),
       card_body(
         plotOutput("plot")
       ))
```

## helpers.R

```{r}
#| label: helpers.R
#| eval: false
# helpers.R
library(dplyr)
library(ggplot2)
library(dbplyr)
library(dbplot)

# Returns binned interest rates for 50 most similar applicants
find_rates_for_similar_applicants <- function(data, 
                                              .term, 
                                              .all_util, 
                                              .bc_util, 
                                              .bc_open_to_buy) {  
    
  data |>
    mutate(distance = (term - as.numeric(.term))^2 +
             (all_util - .all_util)^2 +
             (bc_util - .bc_util)^2 +
             (bc_open_to_buy - .bc_open_to_buy)^2,
           rank = min_rank(distance)) |> 
    filter(rank <= 50 & !is.na(rank)) |> 
    db_compute_bins(int_rate, binwidth = 0.5)
  
}

# Plots binned interest rates
plot_rate_distribution <- function(distribution) {
  
  distribution |>
    ggplot() +
    geom_col(aes(x = int_rate, y = count), 
             fill = "#4682b4", color = "#4682b4", alpha = 0.4) +
    labs(x = "Interest Rate (%)", y = "Number of applicants") +
    scale_x_continuous(limits = c(0, 35),
                       breaks = seq(from = 0, to = 35, by = 5)) +
    theme_minimal()
  
}

```


:::


:::{.column-page}

<iframe src="https://pub.demo.posit.team/public/predicted-interest-rate-calculator/" width="100%" height="700px"></iframe>

:::

In summary, we've gone on a comprehensive journey to predict lending rates using machine learning techniques within the context of financial analysis. We began by accessing our data from Databricks, cleaning our data, and using tidymodels for modeling and evaluation. We developed two predictive models, evaluated their performance, and identified key variables driving interest rate predictions.

Leveraging the power of vetiver, we deployed our model for seamless integration into production environments, such as Shiny apps. This holistic approach can help enhance decision-making processes within the financial domain.

# Learn more about the Databricks x Posit partnership

We believe our products are better together. Learn more about our partnership.

* Visit the [Databricks x Posit Solutions Page](https://posit.co/solutions/databricks/).
* View our [co-presented webinar](https://www.youtube.com/watch?v=iShpyDxzMeE) where we talked about improved productivity for your data teams.
* [Schedule a demo](https://posit.co/schedule-a-call/?booking_calendar__c=Databricks).
