---
title: "Predicting lending rates with Databricks and tidymodels"
format: html
knitr:
  opts_chunk: 
    eval: false
---

Machine learning algorithms are reshaping financial decision-making, changing how the industry understands and manages financial risk. By analyzing vast amounts of data, these advanced algorithms deliver predictive insights that drive informed decisions and expedited client service. One example is in the consumer credit market, where accurately predicting lending rates is critical for customer acquisition and retention.

Through historical application analysis, machine learning enables the capability to offer potential clients personalized interest rates, quickly. This mitigates the chance of losing the customers to faster alternatives while minimizing loan-associated risks. Efficient yet thorough application processing allows you to maintain a competitive edge in the market.

Financial analysts using Databricks can harness its performance and robust data governance capabilities, particularly when working with common datasets stored in Delta Lake. Analysts can use ODBC to link with a SQL warehouse or employ sparklyr to interface with a Databricks cluster. Once the data is accessible, the [tidymodels framework of packages](https://www.tidymodels.org/) offer modeling and machine learning capabilities. With Posit professional tools, analysts can use these tools in managed environments with their preferred IDEs and schedule, share, and scale their models. Analysts can seamlessly combine the data governance strengths of Databricks with the powerful tools offered by Posit, making their work more productive and streamlined.

In this article, we will use publicly accessible loan applicant data from LendingClub to create a machine learning model. Our goal is to develop a personalized interest rate prediction model tailored to individual criteria.

## Connection to Databricks Delta Lake from RStudio

For those with access to Databricks, create the table in your catalog by running the below:

```sql
CREATE TABLE lending_club USING com.databricks.spark.csv OPTIONS(path 'dbfs:/databricks-datasets/lending-club-loan-stats/LoanStats_2018Q2.csv', header "true");
```

Let's start by loading the packages that will be integral to our workflow.

```{r}
#| message: false
# Wrangling packages
library(dplyr)
library(tidyr)

# Visualization packages
library(dbplot)
library(ggplot2)

# Modeling packages
library(ranger)
library(rsample)
library(recipes)
library(parsnip)
library(workflows)
library(yardstick)

# MLOps
library(pins)
library(plumber)
library(vetiver)
```

As previously mentioned, analysts have several options for accessing Databricks data. In this walkthrough, we'll demonstrate connectivity using the [odbc package](https://odbc.r-dbi.org/). The Posit Solutions site provides detailed guidance on using the new [odbc `databricks()` function](https://solutions.posit.co/connections/db/databases/databricks/index.html#using-the-new-odbcdatabricks-function). Essentially, we need to store a Databricks token and host URL in our R environment. The function itself requires only the HTTP path to the SQL warehouse.

```{r}
con <-
  DBI::dbConnect(odbc::databricks(),
                 httpPath = Sys.getenv("HTTP_PATH"))
```

This code snippet establishes a connection using the `con` object. Subsequently, we can use dplyr and dbplyr to navigate to our table. We retrieve the `lendingclub` table using `tbl()` and store it in an object named `lendingclub_dat`.

```{r}
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) 
```

We can use the [dbplot package](https://edgararuiz.github.io/dbplot/) to generate a ggplot without transferring the data into R. This visualization displays the distribution of the variable we aim to predict, interest rate. Loan amounts are plotted on the x-axis, while the frequency of loans at each amount is depicted on the y-axis. It's shown that the majority of loans have an interest rate of less than 20%.

```{r}
#| warning: false
lendingclub_dat |>
  mutate(int_rate = as.numeric(REGEXP_REPLACE(int_rate, "%", ""))) |>
  db_compute_bins(int_rate, binwidth = 0.5) |>
  ggplot() +
  geom_col(
    aes(x = int_rate, y = count),
    fill = "#1B909E",
    color = "#1B909E",
    alpha = 0.4
  ) +
labs(title = "Distribution of interest rate",
     x = "Interest rate",
     y = "Count") +
  theme_minimal()
```

## Data cleaning and feature engineering

The `lendingclub_dat` object resembles a typical data frame, but it's actually a SQL table. With the [dbplyr package](), we can use dplyr syntax to query and manipulate these SQL tables directly in R. Running operations before "collecting" the data leads to significantly faster performance.

Certain columns are imported as characters instead of their intended numeric type. With the `mutate()` function, we can convert them to the correct type. Additionally, `mutate()` allows us to create supplementary variables that could potentially influence interest rates.

```{r}
lendingclub_dat <- 
  dplyr::tbl(con, dbplyr::in_catalog("hive_metastore", "default", "lendingclub")) |> 
  mutate(
    # Convert these columns into numeric
    across(c(starts_with("annual"), starts_with("dti"), starts_with("inq"),  starts_with("mo"), starts_with("mths"), starts_with("num"), starts_with("open"), starts_with("percent"), starts_with("pct"), starts_with("revol"), starts_with("tot"), "all_util", "il_util", "tax_liens",  "loan_amnt", "installment", "pub_rec_bankruptcies", "num_tl_120dpd_2m", "bc_util", "max_bal_bc", "bc_open_to_buy", "acc_open_past_24mths", "avg_cur_bal", "delinq_2yrs", "pub_rec"), ~ as.numeric(.)),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate a loan to income statistic
    loan_to_income = case_when(
      application_type == "Individual" ~ loan_amnt / annual_inc,
      .default = loan_amnt / annual_inc_joint
    ),
    # Calculate the percentage of monthly income the installment payment represents
    installment_pct_inc = case_when(
      application_type == "Individual" ~ installment / (annual_inc / 12),
      .default = installment / (annual_inc_joint / 12)
    ),
    # Calculate the percentage of monthly income the installment payment represents
    adjusted_dti = case_when(
      application_type == "Individual" ~ (loan_amnt + tot_cur_bal) / (annual_inc),
      .default = (loan_amnt + tot_cur_bal) / (annual_inc_joint)
    ),
    #  Calculate utilization on installment accounts excluding mortgage balance
    il_util_ex_mort = case_when(
      total_il_high_credit_limit > 0 ~ total_bal_ex_mort / total_il_high_credit_limit,
      .default = 0
    ),
    # Fill debt to income joint with individual debt to income where missing
    dti_joint = coalesce(dti_joint, dti),
    # Fill annual income joint with individual annual income where missing
    annual_inc_joint = coalesce(annual_inc_joint, annual_inc)) |> 
  collect()
```

With our data cleaned up, it's time to examine the variables we intend to include in the model. We create vectors for conveniently referencing variable categories later if necessary. Then, we compile all the desired columns into one vector named `all_vars` for use in our model.

```{r}
applicant_numeric <- 
  c("annual_inc","dti","age_earliest_cr","loan_amnt", "installment")
applicant_text <- 
  c("emp_title","title")
applicant_categorical <- 
  c("application_type", "emp_length", "term")
credit_numeric <- 
  c("acc_open_past_24mths","avg_cur_bal","bc_open_to_buy","bc_util","delinq_2yrs","open_acc","pub_rec","revol_bal","tot_coll_amt","tot_cur_bal","total_acc","total_rev_hi_lim","num_accts_ever_120_pd","num_actv_bc_tl","num_actv_rev_tl","num_bc_sats","num_bc_tl","num_il_tl", "num_rev_tl_bal_gt_0","pct_tl_nvr_dlq","percent_bc_gt_75","tot_hi_cred_lim","total_bal_ex_mort","total_bc_limit","total_il_high_credit_limit","total_rev_hi_lim","all_util", "loan_to_income", "installment_pct_inc","il_util","il_util_ex_mort","total_bal_il","total_cu_tl")
NUMERIC_VARS_QB_20  <- c("inq_last_6mths","mo_sin_old_il_acct", "mo_sin_old_rev_tl_op", "mo_sin_old_rev_tl_op", "mo_sin_rcnt_tl", "mort_acc", "num_op_rev_tl", "num_rev_accts", "num_sats", "pub_rec", "pub_rec_bankruptcies", "tax_liens", "all_util", "loan_to_income")
NUMERIC_VARS_QB_5 <- 
  c("num_tl_120dpd_2m")
NUMERIC_VARS_QB_10 <- 
  c("mths_since_last_delinq","mths_since_last_major_derog","mths_since_last_record","mths_since_rcnt_il","mths_since_recent_bc","mths_since_recent_bc_dlq","mths_since_recent_inq","mths_since_recent_revol_delinq", "num_tl_90g_dpd_24m","num_tl_op_past_12m")
NUMERIC_VARS_QB_50 <- 
  c("installment","bc_open_to_buy","loan_amnt","total_bc_limit","percent_bc_gt_75")

all_vars <- c(applicant_numeric, applicant_categorical, credit_numeric, NUMERIC_VARS_QB_20, NUMERIC_VARS_QB_5, NUMERIC_VARS_QB_10, NUMERIC_VARS_QB_50)
```

Next, we proceed to select only the relevant columns for our model and remove any missing values in our `int_rate` variable:

```{r}
lendingclub_dat_cols <-
  lendingclub_dat_clean |>
  select(int_rate, all_of(all_vars)) |>
  filter(!is.na(int_rate))
```

To ensure our dataset is free of any missing values, we can run the below:

```{r}
colSums(is.na(lendingclub_dat_cols))
```

We can ensure that no factor columns have fewer than two factors:

```{r}
lendingclub_dat_cols |>
  select(where(is.factor)) %>%
  select(where( ~ nlevels(.) < 2))
```

## Model creation

Now that our dataset is ready, we can begin our modeling process. This entails the creation of test and train datasets where we will train and test our model.

```{r}
set.seed(1234)

train_test_split <- initial_split(lendingclub_dat_cols)
lend_train <- training(train_test_split)
lend_test <- testing(train_test_split)
```

Using tidymodels, we can create a "recipe" that outlines the decisions we want to make for our data.

* `step_normalize()` is a recipe step that normalizes numeric data to have a standard deviation of one and a mean of zero. Since our dataset contains numeric values of different units (dollars, months), normalizing the data avoids overly weighting variables that simply have a larger range of values.
* `step_impute_mean()` is a recipe step that substitute missing values of numeric values by the training set mean of those values. We want to take this step now rather than during the data cleaning section above because taking the mean (or maximum, mode, etc) of the whole dataset means that information from the testing set has leaked into the model. This recipe step will take the mean using only the training set and then apply that mean to the testing data when evaluating our model.

```{r}
rec_obj <- recipe(int_rate ~ ., data = lend_train) |>
  step_normalize(applicant_numeric, credit_numeric) |>
  step_impute_mean(mean_impute_vals)
```

We can check that the recipe does what we hope by running `prep(rec_obj, lend_train) %>% bake(newdata = NULL)`, which shows the data that the workflow will actually hand off to the model.

```{r}
#| warning: false
#| message: false
prep(rec_obj, lend_train) %>% bake(new_data = NULL)
```

Now, we can proceed with our linear model. `linear_reg()` defines a model that can predict numeric values from predictors using a linear function. Then, we create a workflow that adds our recipe to that linear model,

```{r}
lend_linear <- linear_reg()

lend_linear_wflow <-
  workflow() |>
  add_model(lend_linear) |>
  add_recipe(rec_obj)
```

Now, we can fit our model using `fit()` and the training dataset.

```{r}
lend_linear_fit <-
  lend_linear_wflow |>
  fit(data = lend_train)
```

Our predicted interest rates are shown below:

```{r}
predict(lend_linear_fit, lend_test)
```

How did we do? We can analyze our results by calculating the cofficient of determination. It is a value of XX, which is not a very strong estimate.

```{r}
lend_linear_results <-
  bind_cols(predict(lend_linear_fit, lend_train)) |>
  bind_cols(lend_train |>
              select(int_rate))

rsq(lend_linear_results, truth = int_rate, estimate = .pred)
```

However, one of the benefits of tidymodels is being able to quickly pivot with the steps we've already taken. Instead of a linear model, we can define a random forest model:

```{r}
lend_rand <- rand_forest(mode = "regression") |>
  set_engine("ranger",
             importance = "permutation")
```

Then, we can pipe that model into our workflow and fit the model:

```{r}
lend_ranger_wflow <- 
  workflow() |>
  add_model(lend_rand) |>
  add_recipe(rec_obj)

lend_ranger_fit <-
  lend_ranger_wflow |>
  fit(data = lend_train)
```

Now, we can recaulcuate the R squared:

```{r}
lend_ranger_results <-
  bind_cols(predict(lend_ranger_fit, lend_train)) |>
  bind_cols(lend_train |>
              select(int_rate))

rsq(lend_ranger_results, truth = int_rate, estimate = .pred)
```

Now that is a great result!

We had many variables in our model. What if we want to find out which of the variables were the most important? In that case, we can calculate variable imporance by running `vi`. 

```{r}
vip:::vi(lend_ranger_fit) |>
  arrange(desc(Importance))
```

# Model logging and artifact storage

Let's say we want to log our model for use in other places, such as a Shiny app. The [vetiver package] allows us to deploy and maintain machine learning models in production. It allows us to save models in a pin for easy access and reference.

Let's recreate our model using just the top five variables of importance:

```{r}
imp_var <- c("term", "installment_pct_inc", "bc_open_to_buy", "installment", "loan_to_income")

lendingclub_dat_cols_select <- 
  lendingclub_dat_cols |> 
  select(int_rate, all_of(imp_var))

train_test_split_select <- initial_split(lendingclub_dat_cols_select)

lend_train_select <- training(train_test_split_select)
lend_test_select <- testing(train_test_split_select)

rec_obj_select <- recipe(int_rate ~ ., data = lend_train_select) |>
  step_normalize(c("installment", "installment_pct_inc", "loan_to_income")) |>
  step_impute_mean("bc_open_to_buy") |> 
  step_other()

lend_wflow_select <- 
  workflow() |>
  add_model(lend_rand) |>
  add_recipe(rec_obj_select)

lend_fit_select <-
  lend_wflow_select |>
  fit(data = lend_train_select)
```

We can then create a vetiver object `v` for deployment of the trained model:

```{r}
v <- vetiver_model(lend_fit_select, "lend_fit")
```

With the plumber package, we can see what the model would look like:

```{r}
pr() %>%
  vetiver_api(v)
```


Next, we connect to where we will save our model artifact - in our case, Posit Connect - and save the vetiver model to a pin.

```{r}
board <-
  pins::board_connect(auth = "manual",
                      server = Sys.getenv("CONNECT_SERVER"),
                      key = Sys.getenv("CONNECT_API_KEY"))

board %>% vetiver_pin_write(v)
```

Now, we can deploy the model to Posit Connect to be able to reference it and make predictions:

```{r}
vetiver_deploy_rsconnect(
  board = board,
  name = "isabella.velasquez/lend_fit",
  predict_args = list(debug = TRUE)
)
```

